{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhqrhq/Adversarial_Examples_Papers/blob/main/pTRpred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QQYdTXEtekJi"
      },
      "outputs": [],
      "source": [
        "# roll_windows\n",
        "from typing import Sequence, Literal\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def roll_windows(\n",
        "    time: Sequence,\n",
        "    window: int,\n",
        "    step: int = 1,\n",
        "    align: Literal[\"end\", \"center\", \"start\"] = \"end\",\n",
        "    type: Literal[\"rolling\", \"expanding\"] = \"rolling\",\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build rolling / expanding window indices over a time vector.\n",
        "    \"\"\"\n",
        "    if window < 1:\n",
        "        raise ValueError(\"`window` must be >= 1\")\n",
        "    if step < 1:\n",
        "        raise ValueError(\"`step` must be >= 1\")\n",
        "\n",
        "    align = align.lower()\n",
        "    if align not in {\"end\", \"center\", \"start\"}:\n",
        "        raise ValueError(\"`align` must be one of {'end','center','start'}\")\n",
        "\n",
        "    type = type.lower()\n",
        "    if type not in {\"rolling\", \"expanding\"}:\n",
        "        raise ValueError(\"`type` must be one of {'rolling','expanding'}\")\n",
        "\n",
        "    ts = pd.Series(time)\n",
        "    n = len(ts)\n",
        "\n",
        "    if type == \"rolling\":\n",
        "        if n < window:\n",
        "            starts = np.array([1], dtype=int)\n",
        "            ends   = np.array([n], dtype=int)\n",
        "        else:\n",
        "            starts = np.arange(1, n - window + 2, step, dtype=int)\n",
        "            ends   = starts + window - 1\n",
        "    else:\n",
        "        if n < window:\n",
        "            ends = np.array([n], dtype=int)\n",
        "        else:\n",
        "            ends = np.arange(window, n + 1, step, dtype=int)\n",
        "            if ends.size == 0:\n",
        "                ends = np.array([n], dtype=int)\n",
        "        starts = np.full_like(ends, 1)\n",
        "\n",
        "    if align == \"end\":\n",
        "        t_rep_idx_1b = ends\n",
        "    elif align == \"start\":\n",
        "        t_rep_idx_1b = starts\n",
        "    else:\n",
        "        t_rep_idx_1b = np.floor((starts + ends) / 2).astype(int)\n",
        "\n",
        "    t_rep = ts.iloc[t_rep_idx_1b - 1].to_numpy()\n",
        "    idx_list = [list(range(int(s), int(e) + 1)) for s, e in zip(starts, ends)]\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"window_id\": np.arange(1, len(starts) + 1, dtype=int),\n",
        "        \"start\": starts,\n",
        "        \"end\": ends,\n",
        "        \"t_rep\": t_rep,\n",
        "        \"idx\": idx_list,\n",
        "    })\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_X\n",
        "import pandas as pd\n",
        "from typing import Sequence, Union\n",
        "\n",
        "def extract_X(data: pd.DataFrame, x_cols: Union[Sequence[str], str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extract a numeric matrix (as a pandas DataFrame) from a data frame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        Must contain the columns of interest.\n",
        "    x_cols : list of str or str\n",
        "        - A list of exact column names, e.g. [\"temp1\",\"temp2\"]\n",
        "        - A single regex string containing '|' to match multiple names, e.g. \"temp|volt\"\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Numeric DataFrame with preserved column names.\n",
        "    \"\"\"\n",
        "    if not isinstance(data, pd.DataFrame):\n",
        "        raise ValueError(\"`data` must be a pandas DataFrame.\")\n",
        "\n",
        "    if isinstance(x_cols, str):\n",
        "        if \"|\" in x_cols:\n",
        "            keep = data.filter(regex=x_cols).columns.tolist()\n",
        "        else:\n",
        "            keep = [x_cols]\n",
        "    elif isinstance(x_cols, (list, tuple)):\n",
        "        keep = list(x_cols)\n",
        "    else:\n",
        "        raise ValueError(\"`x_cols` must be a list of names or a single regex string.\")\n",
        "\n",
        "    if len(keep) == 0:\n",
        "        raise ValueError(\"No columns selected by `x_cols`.\")\n",
        "\n",
        "    X = data.loc[:, keep].copy()\n",
        "    for c in keep:\n",
        "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "0O8n36dcfIqe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# na_handle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Literal, Union\n",
        "\n",
        "def na_handle(\n",
        "    M: Union[np.ndarray, pd.DataFrame],\n",
        "    method: Literal[\"omit_rows\",\"impute_mean\",\"pairwise_complete\"] = \"omit_rows\"\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Simple NA handling for numeric matrices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    M : array-like (numpy array or pandas DataFrame)\n",
        "        Numeric-like object.\n",
        "    method : {\"omit_rows\",\"impute_mean\",\"pairwise_complete\"}\n",
        "        - \"omit_rows\": drop any rows containing NaN\n",
        "        - \"impute_mean\": impute NaNs with column means (if >=2 non-NA values)\n",
        "        - \"pairwise_complete\": leave NAs as-is\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "        Numeric matrix with NAs handled.\n",
        "    \"\"\"\n",
        "    if isinstance(M, pd.DataFrame):\n",
        "        A = M.to_numpy(dtype=float)\n",
        "    else:\n",
        "        A = np.asarray(M, dtype=float)\n",
        "\n",
        "    if method == \"omit_rows\":\n",
        "        mask = ~np.isnan(A).any(axis=1)\n",
        "        return A[mask, :]\n",
        "\n",
        "    if method == \"impute_mean\":\n",
        "        A_copy = A.copy()\n",
        "        for j in range(A_copy.shape[1]):\n",
        "            col = A_copy[:, j]\n",
        "            nan_mask = np.isnan(col)\n",
        "            if not nan_mask.any():\n",
        "                continue\n",
        "            n_non_na = np.sum(~nan_mask)\n",
        "            if n_non_na <= 1:\n",
        "                # 0 or 1 observed values -> leave NaNs as NaN\n",
        "                continue\n",
        "            m = np.nanmean(col)\n",
        "            A_copy[nan_mask, j] = m\n",
        "        return A_copy\n",
        "\n",
        "    if method == \"pairwise_complete\":\n",
        "        return A\n",
        "\n",
        "    raise ValueError(\"`method` must be one of {'omit_rows','impute_mean','pairwise_complete'}\")"
      ],
      "metadata": {
        "id": "CFJ-oIZzgFwi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roll_svd\n",
        "from typing import List, Optional, Sequence, Literal, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assumes roll_windows, extract_X, and na_handle are already defined.\n",
        "\n",
        "def _cov_window(\n",
        "    Xc: pd.DataFrame,\n",
        "    na_action: Literal[\"omit_rows\",\"impute_mean\",\"pairwise_complete\"],\n",
        "    cov_on_pairwise: bool\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute covariance for a single window, honoring pairwise-complete option.\n",
        "    Returns a (p x p) numpy array.\n",
        "    \"\"\"\n",
        "    if na_action == \"pairwise_complete\" and cov_on_pairwise:\n",
        "        S = Xc.cov(min_periods=2)  # pairwise complete by default\n",
        "        return S.to_numpy()\n",
        "    else:\n",
        "        Xc2 = Xc.dropna(axis=0, how=\"any\")\n",
        "        if Xc2.shape[0] < 2:\n",
        "            return np.full((Xc.shape[1], Xc.shape[1]), np.nan, dtype=float)\n",
        "        return np.cov(Xc2.to_numpy(), rowvar=False, ddof=1)\n",
        "\n",
        "def roll_svd(\n",
        "    data: pd.DataFrame,\n",
        "    time: str,\n",
        "    x_cols: Sequence[str] | str,\n",
        "    window: int,\n",
        "    step: int = 1,\n",
        "    align: Literal[\"end\", \"center\", \"start\"] = \"end\",\n",
        "    type: Literal[\"rolling\", \"expanding\"] = \"rolling\",\n",
        "    center: bool = True,\n",
        "    scale_: bool = False,\n",
        "    k: Optional[int] = None,\n",
        "    fast: bool = True,\n",
        "    na_action: Literal[\"omit_rows\",\"impute_mean\",\"pairwise_complete\"] = \"omit_rows\",\n",
        "    cov_on_pairwise: bool = True,\n",
        "    values_only: bool = True,\n",
        "    seed: Optional[int] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Rolling SVD (eigendecomposition) of per-window covariance.\n",
        "\n",
        "    Returns a dict with:\n",
        "      - windows   : DataFrame from roll_windows (one row per window)\n",
        "      - D         : list of 1D numpy arrays (eigenvalues per window)\n",
        "      - V         : list of (p x k) numpy arrays (eigenvectors) or None if values_only\n",
        "      - U_scores  : list of (n_w x k) numpy arrays (standardized scores) or None if values_only\n",
        "      - k         : requested k (or None if not set)\n",
        "      - colnames  : list of feature names\n",
        "      - preproc   : {'center': bool, 'scale.': bool}\n",
        "      - values_only : bool\n",
        "    \"\"\"\n",
        "    if time not in data.columns:\n",
        "        raise ValueError(\"`time` must be a column in `data`.\")\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    windows = roll_windows(data[time].to_numpy(), window, step, align, type)\n",
        "    X_all = extract_X(data, x_cols)\n",
        "    colnames = list(X_all.columns)\n",
        "    p = X_all.shape[1]\n",
        "\n",
        "    nW = windows.shape[0]\n",
        "    D_list: List[Optional[np.ndarray]] = [None] * nW\n",
        "    V_list: Optional[List[Optional[np.ndarray]]] = None if values_only else [None] * nW\n",
        "    U_list: Optional[List[Optional[np.ndarray]]] = None if values_only else [None] * nW\n",
        "\n",
        "    _have_scipy = False\n",
        "    if fast:\n",
        "        try:\n",
        "            from scipy.sparse.linalg import eigsh  # type: ignore\n",
        "            _have_scipy = True\n",
        "        except Exception:\n",
        "            _have_scipy = False\n",
        "\n",
        "    for w in range(nW):\n",
        "        idx_1b = windows.iloc[w][\"idx\"]          # list of 1-based indices\n",
        "        idx_0b = [i - 1 for i in idx_1b]         # convert to 0-based\n",
        "        Xw = X_all.iloc[idx_0b, :].copy()\n",
        "\n",
        "        # NA handling (per window)\n",
        "        if na_action == \"omit_rows\":\n",
        "            Xw = Xw.dropna(axis=0, how=\"any\")\n",
        "        elif na_action == \"impute_mean\":\n",
        "            Xw = Xw.copy()\n",
        "            for c in Xw.columns:\n",
        "                arr = Xw[c].to_numpy()\n",
        "                nan_mask = np.isnan(arr)\n",
        "                if nan_mask.any():\n",
        "                    n_non_na = (~nan_mask).sum()\n",
        "                    if n_non_na > 1:\n",
        "                        Xw.loc[nan_mask, c] = np.nanmean(arr)\n",
        "        elif na_action == \"pairwise_complete\":\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"`na_action` must be one of {'omit_rows','impute_mean','pairwise_complete'}\")\n",
        "\n",
        "        n_w = Xw.shape[0]\n",
        "        if n_w < 2:\n",
        "            continue  # skip this window; entries remain None\n",
        "\n",
        "        # Center/scale per window\n",
        "        if center:\n",
        "            mu = Xw.mean(axis=0, skipna=True)\n",
        "        else:\n",
        "            mu = pd.Series(np.zeros(p), index=Xw.columns)\n",
        "\n",
        "        if scale_:\n",
        "            sc = Xw.std(axis=0, ddof=1, skipna=True).replace(0, np.nan)\n",
        "        else:\n",
        "            sc = pd.Series(np.ones(p), index=Xw.columns)\n",
        "\n",
        "        sc = sc.fillna(1.0)\n",
        "        Xc = (Xw - mu) / sc\n",
        "\n",
        "        # Effective k for this window\n",
        "        kk = min(k if k is not None else min(n_w, p), n_w, p)\n",
        "\n",
        "        # Covariance for this window\n",
        "        S = _cov_window(Xc, na_action, cov_on_pairwise)\n",
        "        if not np.isfinite(S).all():\n",
        "            continue\n",
        "\n",
        "        if values_only:\n",
        "            vals = np.linalg.eigvalsh(S)[::-1]  # descending\n",
        "            D_list[w] = vals[:kk].copy()\n",
        "        else:\n",
        "            if _have_scipy and kk < p:\n",
        "                from scipy.sparse.linalg import eigsh  # type: ignore\n",
        "                vals, vecs = eigsh(S, k=kk, which=\"LA\")\n",
        "                order = np.argsort(-vals)\n",
        "                vals = vals[order]\n",
        "                vecs = vecs[:, order]\n",
        "            else:\n",
        "                vals_full, vecs_full = np.linalg.eigh(S)  # ascending\n",
        "                order = np.argsort(-vals_full)\n",
        "                vals = vals_full[order][:kk]\n",
        "                vecs = vecs_full[:, order][:, :kk]\n",
        "\n",
        "            D_list[w] = vals.copy()\n",
        "            V_list[w] = vecs.copy() if V_list is not None else None\n",
        "            if U_list is not None:\n",
        "                U_list[w] = Xc.to_numpy() @ vecs\n",
        "\n",
        "    return {\n",
        "        \"windows\": windows,\n",
        "        \"D\": D_list,\n",
        "        \"V\": V_list,\n",
        "        \"U_scores\": U_list,\n",
        "        \"k\": (None if k is None else int(k)),\n",
        "        \"colnames\": colnames,\n",
        "        \"preproc\": {\"center\": bool(center), \"scale.\": bool(scale_)},\n",
        "        \"values_only\": bool(values_only),\n",
        "    }"
      ],
      "metadata": {
        "id": "hxlcf3v_ghKV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit_arimax_vec\n",
        "import numpy as np\n",
        "from typing import Optional, Dict, Any\n",
        "import warnings\n",
        "\n",
        "def fit_arimax_vec(\n",
        "    y,\n",
        "    xreg: Optional[np.ndarray] = None,\n",
        "    seasonal: bool = True,\n",
        "    stepwise: bool = True,\n",
        "    approximation: bool = False,\n",
        "    **kwargs: Any,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Fit a single ARIMAX model (auto-selected order) and return aligned residuals/fitted.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : array-like\n",
        "        Dependent (response) series.\n",
        "    xreg : array-like, optional\n",
        "        Exogenous regressors (rows must align with y).\n",
        "    seasonal : bool, default True\n",
        "        Passed to pmdarima.auto_arima.\n",
        "    stepwise : bool, default True\n",
        "        Passed to pmdarima.auto_arima.\n",
        "    approximation : bool, default False\n",
        "        Passed to pmdarima.auto_arima.\n",
        "    **kwargs :\n",
        "        Additional arguments forwarded to pmdarima.auto_arima().\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict with keys:\n",
        "      - model      : fitted pmdarima model object\n",
        "      - residuals  : np.ndarray (residuals for the rows used)\n",
        "      - fitted     : np.ndarray (fitted values for the rows used)\n",
        "      - mask       : np.ndarray of bools (length len(y)), True where rows were used\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import pmdarima as pm\n",
        "    except Exception as e:\n",
        "        raise ImportError(\n",
        "            \"Package 'pmdarima' is required but not installed. \"\n",
        "            \"Install it with: pip install pmdarima\"\n",
        "        ) from e\n",
        "\n",
        "    y_arr = np.asarray(y, dtype=float).reshape(-1)\n",
        "    n = y_arr.shape[0]\n",
        "\n",
        "    if xreg is not None:\n",
        "        X = np.asarray(xreg, dtype=float)\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(-1, 1)\n",
        "        if X.shape[0] != n:\n",
        "            raise ValueError(\"`xreg` must have the same number of rows as `y`.\")\n",
        "        ok = np.isfinite(y_arr) & np.isfinite(X).all(axis=1)\n",
        "        X_used = X[ok, :]\n",
        "    else:\n",
        "        ok = np.isfinite(y_arr)\n",
        "        X_used = None\n",
        "\n",
        "    if not np.any(ok):\n",
        "        raise ValueError(\"No complete cases available for ARIMAX fit.\")\n",
        "\n",
        "    y_used = y_arr[ok]\n",
        "\n",
        "    # Silence convergence/numerical warnings unless user wants them\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        model = pm.auto_arima(\n",
        "            y=y_used,\n",
        "            X=X_used,\n",
        "            seasonal=seasonal,\n",
        "            stepwise=stepwise,\n",
        "            approximation=approximation,\n",
        "            error_action=\"warn\",\n",
        "            suppress_warnings=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    # In-sample predictions and residuals on the used subset\n",
        "    try:\n",
        "        fitted_used = model.predict_in_sample(X=X_used)\n",
        "    except TypeError:\n",
        "        # older pmdarima signature may use 'exogenous'\n",
        "        fitted_used = model.predict_in_sample(exogenous=X_used)\n",
        "\n",
        "    # Some versions expose residuals via .resid(), others via attribute\n",
        "    try:\n",
        "        resid_used = model.resid()\n",
        "    except Exception:\n",
        "        resid_used = y_used - np.asarray(fitted_used, dtype=float)\n",
        "\n",
        "    fitted_used = np.asarray(fitted_used, dtype=float).reshape(-1)\n",
        "    resid_used = np.asarray(resid_used, dtype=float).reshape(-1)\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"residuals\": resid_used,\n",
        "        \"fitted\": fitted_used,\n",
        "        \"mask\": ok,\n",
        "    }"
      ],
      "metadata": {
        "id": "vK-VxYZrhWf8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Sequence, Dict, Any, Optional\n",
        "\n",
        "# Assumes extract_X(...) and fit_arimax_vec(...) are defined.\n",
        "\n",
        "def arimax_residuals_df(\n",
        "    data: pd.DataFrame,\n",
        "    time: str,\n",
        "    y_cols: Sequence[str] | str,\n",
        "    xreg_cols: Sequence[str] | str,\n",
        "    seasonal: bool = True,\n",
        "    stepwise: bool = True,\n",
        "    approximation: bool = False,\n",
        "    **kwargs: Any,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Batch ARIMAX residuals for multiple series with a common regressor set.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict with keys:\n",
        "      - residuals_df : DataFrame with columns [time] + one column per y series\n",
        "      - models       : dict of fitted models keyed by series name\n",
        "      - mask         : boolean mask (length nrow(data)) for rows used (complete cases)\n",
        "    \"\"\"\n",
        "    if time not in data.columns:\n",
        "        raise ValueError(\"`time` must be a column in `data`.\")\n",
        "\n",
        "    Ymat = extract_X(data, y_cols)          # n x p_y\n",
        "    Xreg = extract_X(data, xreg_cols)       # n x p_x\n",
        "    tvec = data[time].to_numpy()\n",
        "\n",
        "    # Global complete-case mask across all selected y and xreg columns\n",
        "    ok = np.isfinite(Ymat.to_numpy()).all(axis=1) & np.isfinite(Xreg.to_numpy()).all(axis=1)\n",
        "    if not np.any(ok):\n",
        "        raise ValueError(\"No complete cases across y_cols and xreg_cols for ARIMAX.\")\n",
        "\n",
        "    Y_ok = Ymat.loc[ok, :].reset_index(drop=True)\n",
        "    X_ok = Xreg.loc[ok, :].reset_index(drop=True)\n",
        "    t_ok = tvec[ok]\n",
        "\n",
        "    n_ok, p_y = Y_ok.shape\n",
        "    Resids = np.full((n_ok, p_y), np.nan, dtype=float)\n",
        "    models: Dict[str, Any] = {}\n",
        "\n",
        "    for j, col in enumerate(Y_ok.columns):\n",
        "        fitj = fit_arimax_vec(\n",
        "            y=Y_ok[col].to_numpy(),\n",
        "            xreg=X_ok.to_numpy(),\n",
        "            seasonal=seasonal,\n",
        "            stepwise=stepwise,\n",
        "            approximation=approximation,\n",
        "            **kwargs,\n",
        "        )\n",
        "        inner_ok = fitj[\"mask\"]  # mask relative to Y_ok (length n_ok)\n",
        "        Resids[inner_ok, j] = fitj[\"residuals\"]\n",
        "        models[col] = fitj[\"model\"]\n",
        "\n",
        "    residuals_df = pd.DataFrame({\"time\": t_ok})\n",
        "    for j, col in enumerate(Y_ok.columns):\n",
        "        residuals_df[col] = Resids[:, j]\n",
        "\n",
        "    return {\"residuals_df\": residuals_df, \"models\": models, \"mask\": ok}"
      ],
      "metadata": {
        "id": "34waaD8whrYP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# arimax_residuals_df\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Sequence, Dict, Any, Optional\n",
        "\n",
        "# Assumes extract_X(...) and fit_arimax_vec(...) are defined.\n",
        "\n",
        "def arimax_residuals_df(\n",
        "    data: pd.DataFrame,\n",
        "    time: str,\n",
        "    y_cols: Sequence[str] | str,\n",
        "    xreg_cols: Sequence[str] | str,\n",
        "    seasonal: bool = True,\n",
        "    stepwise: bool = True,\n",
        "    approximation: bool = False,\n",
        "    **kwargs: Any,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Batch ARIMAX residuals for multiple series with a common regressor set.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict with keys:\n",
        "      - residuals_df : DataFrame with columns [time] + one column per y series\n",
        "      - models       : dict of fitted models keyed by series name\n",
        "      - mask         : boolean mask (length nrow(data)) for rows used (complete cases)\n",
        "    \"\"\"\n",
        "    if time not in data.columns:\n",
        "        raise ValueError(\"`time` must be a column in `data`.\")\n",
        "\n",
        "    Ymat = extract_X(data, y_cols)          # n x p_y\n",
        "    Xreg = extract_X(data, xreg_cols)       # n x p_x\n",
        "    tvec = data[time].to_numpy()\n",
        "\n",
        "    # Global complete-case mask across all selected y and xreg columns\n",
        "    ok = np.isfinite(Ymat.to_numpy()).all(axis=1) & np.isfinite(Xreg.to_numpy()).all(axis=1)\n",
        "    if not np.any(ok):\n",
        "        raise ValueError(\"No complete cases across y_cols and xreg_cols for ARIMAX.\")\n",
        "\n",
        "    Y_ok = Ymat.loc[ok, :].reset_index(drop=True)\n",
        "    X_ok = Xreg.loc[ok, :].reset_index(drop=True)\n",
        "    t_ok = tvec[ok]\n",
        "\n",
        "    n_ok, p_y = Y_ok.shape\n",
        "    Resids = np.full((n_ok, p_y), np.nan, dtype=float)\n",
        "    models: Dict[str, Any] = {}\n",
        "\n",
        "    for j, col in enumerate(Y_ok.columns):\n",
        "        fitj = fit_arimax_vec(\n",
        "            y=Y_ok[col].to_numpy(),\n",
        "            xreg=X_ok.to_numpy(),\n",
        "            seasonal=seasonal,\n",
        "            stepwise=stepwise,\n",
        "            approximation=approximation,\n",
        "            **kwargs,\n",
        "        )\n",
        "        inner_ok = fitj[\"mask\"]  # mask relative to Y_ok (length n_ok)\n",
        "        Resids[inner_ok, j] = fitj[\"residuals\"]\n",
        "        models[col] = fitj[\"model\"]\n",
        "\n",
        "    residuals_df = pd.DataFrame({\"time\": t_ok})\n",
        "    for j, col in enumerate(Y_ok.columns):\n",
        "        residuals_df[col] = Resids[:, j]\n",
        "\n",
        "    return {\"residuals_df\": residuals_df, \"models\": models, \"mask\": ok}"
      ],
      "metadata": {
        "id": "-IQjcqSRjqre"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# arimax_then_roll_svd\n",
        "import pandas as pd\n",
        "from typing import Sequence, Dict, Any, Optional, Literal\n",
        "\n",
        "# Assumes arimax_residuals_df(...) and roll_svd(...) are defined.\n",
        "\n",
        "def arimax_then_roll_svd(\n",
        "    data: pd.DataFrame,\n",
        "    time: str,\n",
        "    y_cols: Sequence[str] | str,\n",
        "    xreg_cols: Sequence[str] | str,\n",
        "    window: int,\n",
        "    step: int = 1,\n",
        "    align: Literal[\"end\", \"center\", \"start\"] = \"end\",\n",
        "    type: Literal[\"rolling\", \"expanding\"] = \"rolling\",\n",
        "    center: bool = True,\n",
        "    scale_: bool = False,\n",
        "    k: Optional[int] = None,\n",
        "    fast: bool = True,\n",
        "    na_action: Literal[\"omit_rows\",\"impute_mean\",\"pairwise_complete\"] = \"omit_rows\",\n",
        "    cov_on_pairwise: bool = True,\n",
        "    seasonal: bool = True,\n",
        "    stepwise: bool = True,\n",
        "    approximation: bool = False,\n",
        "    values_only: bool = True,\n",
        "    seed: Optional[int] = None,\n",
        "    **kwargs: Any,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Pipeline: ARIMAX residuals → rolling SVD on residual covariance.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict with keys:\n",
        "      - residuals_df : DataFrame of aligned ARIMAX residuals with a 'time' column\n",
        "      - rollsvd      : dict returned by roll_svd(...)\n",
        "      - models       : dict of fitted ARIMAX models keyed by series name\n",
        "      - mask         : boolean mask over original rows used in ARIMAX fits\n",
        "    \"\"\"\n",
        "    fit = arimax_residuals_df(\n",
        "        data=data,\n",
        "        time=time,\n",
        "        y_cols=y_cols,\n",
        "        xreg_cols=xreg_cols,\n",
        "        seasonal=seasonal,\n",
        "        stepwise=stepwise,\n",
        "        approximation=approximation,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    residuals_df = fit[\"residuals_df\"]\n",
        "    # Feature columns are everything after 'time'\n",
        "    res_cols = [c for c in residuals_df.columns if c != \"time\"]\n",
        "\n",
        "    svd_fit = roll_svd(\n",
        "        data=residuals_df,\n",
        "        time=\"time\",\n",
        "        x_cols=res_cols,                 # pass explicit list of columns\n",
        "        window=window,\n",
        "        step=step,\n",
        "        align=align,\n",
        "        type=type,\n",
        "        center=center,\n",
        "        scale_=scale_,\n",
        "        k=k,\n",
        "        fast=fast,\n",
        "        na_action=na_action,\n",
        "        cov_on_pairwise=cov_on_pairwise,\n",
        "        values_only=values_only,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"residuals_df\": residuals_df,\n",
        "        \"rollsvd\": svd_fit,\n",
        "        \"models\": fit[\"models\"],\n",
        "        \"mask\": fit[\"mask\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "ZQrCr-mFjs45"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as_tibble_rollsvd\n",
        "import pandas as pd\n",
        "from typing import Literal, Dict, Any, List, Optional\n",
        "\n",
        "def as_tibble_rollsvd(\n",
        "    x: Dict[str, Any],\n",
        "    what: Literal[\"loadings\", \"singular_values\"] = \"singular_values\",\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Tidy a rollsvd object (dict from roll_svd) into a long DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : dict\n",
        "        Output from roll_svd(...).\n",
        "    what : {\"loadings\",\"singular_values\"}, default \"singular_values\"\n",
        "        - \"loadings\": requires roll_svd(..., values_only=False).\n",
        "          Returns columns: window_id, variable, factor, loading.\n",
        "        - \"singular_values\": always available.\n",
        "          Returns columns: window_id, factor, d.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame or None\n",
        "        Long-format DataFrame, or None if no windows contained results.\n",
        "    \"\"\"\n",
        "    if not isinstance(x, dict) or \"D\" not in x:\n",
        "        raise ValueError(\"`x` must be a dict returned by roll_svd (with key 'D').\")\n",
        "\n",
        "    V_list: Optional[List[Optional[Any]]] = x.get(\"V\", None)\n",
        "    D_list: List[Optional[Any]] = x.get(\"D\", [])\n",
        "    colnames: List[str] = x.get(\"colnames\", [])\n",
        "\n",
        "    if V_list is not None:\n",
        "        W = len(V_list)\n",
        "    elif D_list is not None:\n",
        "        W = len(D_list)\n",
        "    else:\n",
        "        W = 0\n",
        "\n",
        "    if W == 0:\n",
        "        return None\n",
        "\n",
        "    out_frames: List[pd.DataFrame] = []\n",
        "\n",
        "    if what == \"loadings\":\n",
        "        if V_list is None:\n",
        "            raise ValueError(\"Loadings not available: roll_svd was run with values_only=True.\")\n",
        "        for w in range(W):\n",
        "            V = V_list[w]\n",
        "            if V is None:\n",
        "                continue\n",
        "            p, k = V.shape\n",
        "            dfw = pd.DataFrame({\n",
        "                \"window_id\": (w + 1),\n",
        "                \"variable\": colnames * k,\n",
        "                \"factor\": sum(([i + 1] * p for i in range(k)), []),\n",
        "                \"loading\": V.reshape(-1, order=\"F\"),  # match as.vector(column-major)\n",
        "            })\n",
        "            out_frames.append(dfw)\n",
        "    elif what == \"singular_values\":\n",
        "        for w in range(W):\n",
        "            D = D_list[w]\n",
        "            if D is None:\n",
        "                continue\n",
        "            dfw = pd.DataFrame({\n",
        "                \"window_id\": (w + 1),\n",
        "                \"factor\": list(range(1, len(D) + 1)),\n",
        "                \"d\": list(map(float, D)),\n",
        "            })\n",
        "            out_frames.append(dfw)\n",
        "    else:\n",
        "        raise ValueError(\"`what` must be 'loadings' or 'singular_values'.\")\n",
        "\n",
        "    if not out_frames:\n",
        "        return None\n",
        "\n",
        "    out = pd.concat(out_frames, axis=0, ignore_index=True)\n",
        "    return out"
      ],
      "metadata": {
        "id": "HIgxh96xkRJK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# signals\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Sequence, Literal, Optional, Any\n",
        "\n",
        "# Assumes: roll_svd, extract_X, fit_arimax_vec, arimax_residuals_df are defined.\n",
        "\n",
        "def build_signal_raw(data: pd.DataFrame, time: str, col: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Raw single-column series → (time_ind, signal)\n",
        "    \"\"\"\n",
        "    if time not in data.columns:\n",
        "        raise ValueError(\"`time` must be a column in `data`.\")\n",
        "    if col not in data.columns:\n",
        "        raise ValueError(\"`col` must be a column in `data`.\")\n",
        "    return pd.DataFrame({\n",
        "        \"time_ind\": data[time].to_numpy(),\n",
        "        \"signal\": pd.to_numeric(data[col], errors=\"coerce\").to_numpy(),\n",
        "    })\n",
        "\n",
        "def build_signal_svd(\n",
        "    data: pd.DataFrame,\n",
        "    time: str,\n",
        "    x_cols: Sequence[str] | str,\n",
        "    window: int,\n",
        "    step: int = 1,\n",
        "    align: Literal[\"end\", \"center\", \"start\"] = \"end\",\n",
        "    type: Literal[\"rolling\", \"expanding\"] = \"rolling\",\n",
        "    center: bool = True,\n",
        "    scale_: bool = True,\n",
        "    fast: bool = True,\n",
        "    na_action: Literal[\"omit_rows\",\"impute_mean\",\"pairwise_complete\"] = \"omit_rows\",\n",
        "    cov_on_pairwise: bool = True,\n",
        "    seed: Optional[int] = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Rolling SVD top eigenvalue (s1) → (t_rep, s1)\n",
        "    \"\"\"\n",
        "    fit = roll_svd(\n",
        "        data=data, time=time, x_cols=x_cols,\n",
        "        window=window, step=step, align=align, type=type,\n",
        "        center=center, scale_=scale_, k=1,\n",
        "        fast=fast, na_action=na_action, cov_on_pairwise=cov_on_pairwise,\n",
        "        values_only=True, seed=seed\n",
        "    )\n",
        "    # Extract s1 per window; windows without values -> NaN\n",
        "    s1 = [ (np.nan if (d is None or len(d) < 1) else float(d[0])) for d in fit[\"D\"] ]\n",
        "    return pd.DataFrame({\n",
        "        \"time_ind\": fit[\"windows\"][\"t_rep\"].to_numpy(),\n",
        "        \"signal\": np.asarray(s1, dtype=float)\n",
        "    })\n",
        "\n",
        "def build_signal_arimax_resid(\n",
        "    data: pd.DataFrame,\n",
        "    time: str,\n",
        "    y_col: Sequence[str] | str,\n",
        "    xreg_cols: Sequence[str] | str,\n",
        "    seasonal: bool = True,\n",
        "    stepwise: bool = True,\n",
        "    approximation: bool = False,\n",
        "    **kwargs: Any,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    ARIMAX residuals (single y + predictors) → (time_ind, residual)\n",
        "    \"\"\"\n",
        "    if time not in data.columns:\n",
        "        raise ValueError(\"`time` must be a column in `data`.\")\n",
        "\n",
        "    y_df = extract_X(data, y_col)\n",
        "    if y_df.shape[1] != 1:\n",
        "        raise ValueError(\"`y_col` must select exactly one column.\")\n",
        "    X = extract_X(data, xreg_cols)\n",
        "\n",
        "    t = data[time].to_numpy()\n",
        "    fit = fit_arimax_vec(\n",
        "        y=y_df.iloc[:, 0].to_numpy(),\n",
        "        xreg=X.to_numpy(),\n",
        "        seasonal=seasonal,\n",
        "        stepwise=stepwise,\n",
        "        approximation=approximation,\n",
        "        **kwargs\n",
        "    )\n",
        "    return pd.DataFrame({\n",
        "        \"time_ind\": t[fit[\"mask\"]],\n",
        "        \"signal\": fit[\"residuals\"]\n",
        "    })\n",
        "\n",
        "def build_signal_arimax_svd(\n",
        "    data: pd.DataFrame,\n",
        "    time: str,\n",
        "    y_cols: Sequence[str] | str,\n",
        "    xreg_cols: Sequence[str] | str,\n",
        "    window: int,\n",
        "    step: int = 1,\n",
        "    align: Literal[\"end\", \"center\", \"start\"] = \"end\",\n",
        "    type: Literal[\"rolling\", \"expanding\"] = \"rolling\",\n",
        "    center: bool = True,\n",
        "    scale_: bool = True,\n",
        "    fast: bool = True,\n",
        "    na_action: Literal[\"omit_rows\",\"impute_mean\",\"pairwise_complete\"] = \"omit_rows\",\n",
        "    cov_on_pairwise: bool = True,\n",
        "    seasonal: bool = True,\n",
        "    stepwise: bool = True,\n",
        "    approximation: bool = False,\n",
        "    seed: Optional[int] = None,\n",
        "    **kwargs: Any,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    ARIMAX residuals for multiple series → rolling SVD s1 → (t_rep, s1)\n",
        "    \"\"\"\n",
        "    # 1) residuals (aligned by common mask)\n",
        "    arx = arimax_residuals_df(\n",
        "        data=data, time=time, y_cols=y_cols, xreg_cols=xreg_cols,\n",
        "        seasonal=seasonal, stepwise=stepwise, approximation=approximation, **kwargs\n",
        "    )\n",
        "\n",
        "    # 2) SVD on residuals (s1 only)\n",
        "    residuals_df = arx[\"residuals_df\"]\n",
        "    res_cols = [c for c in residuals_df.columns if c != \"time\"]\n",
        "\n",
        "    fit = roll_svd(\n",
        "        data=residuals_df, time=\"time\", x_cols=res_cols,\n",
        "        window=window, step=step, align=align, type=type,\n",
        "        center=center, scale_=scale_, k=1,\n",
        "        fast=fast, na_action=na_action, cov_on_pairwise=cov_on_pairwise,\n",
        "        values_only=True, seed=seed\n",
        "    )\n",
        "\n",
        "    s1 = [ (np.nan if (d is None or len(d) < 1) else float(d[0])) for d in fit[\"D\"] ]\n",
        "    return pd.DataFrame({\n",
        "        \"time_ind\": fit[\"windows\"][\"t_rep\"].to_numpy(),\n",
        "        \"signal\": np.asarray(s1, dtype=float)\n",
        "    })"
      ],
      "metadata": {
        "id": "ey6PgG5mklMt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detect_asvotes\n",
        "import numpy as np\n",
        "from typing import Literal, Union, Sequence\n",
        "\n",
        "def detect_asvotes(\n",
        "    signal: Union[Sequence[float], np.ndarray],\n",
        "    lowwl: int = 5,\n",
        "    highwl: Union[str, int] = \"auto\",\n",
        "    mad_k: float = 3.0,\n",
        "    direction: Literal[\"positive\", \"both\", \"negative\"] = \"positive\",\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Multi-scale robust slope voting detector (AS-votes).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : array-like\n",
        "        Univariate time series.\n",
        "    lowwl : int, default 5\n",
        "        Minimum window length to scan.\n",
        "    highwl : {\"auto\", int}, default \"auto\"\n",
        "        If \"auto\", uses floor(n/3). If int, coerced and clipped to be >= lowwl.\n",
        "    mad_k : float, default 3.0\n",
        "        Threshold in robust median/MAD z-score units to flag outlying slopes.\n",
        "    direction : {\"positive\",\"both\",\"negative\"}, default \"positive\"\n",
        "        Which slope outliers contribute votes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        Vector of length n with normalized vote score per index.\n",
        "    \"\"\"\n",
        "    y = np.asarray(signal, dtype=float).reshape(-1)\n",
        "    n = y.size\n",
        "    if n < 3:\n",
        "        return np.zeros(n, dtype=float)\n",
        "\n",
        "    if isinstance(highwl, str) and highwl.lower() == \"auto\":\n",
        "        highwl_val = max(5, n // 3)\n",
        "    else:\n",
        "        highwl_val = int(highwl)\n",
        "\n",
        "    lowwl_val = int(lowlw := max(2, int(lowlwl)))\n",
        "    if highwl_val < lowwl_val:\n",
        "        highwl_val = lowwl_val\n",
        "\n",
        "    wls = np.arange(lowlw, highwl_val + 1, dtype=int)\n",
        "    votes = np.zeros(n, dtype=float)\n",
        "    n_wls = wls.size\n",
        "\n",
        "    def robust_z(x: np.ndarray) -> np.ndarray:\n",
        "        med = np.nanmedian(x)\n",
        "        mad = np.nanmedian(np.abs(x - med))\n",
        "        if not np.isfinite(mad) or mad == 0:\n",
        "            mad = 1e-8\n",
        "        return (x - med) / mad\n",
        "\n",
        "    for w in wls:\n",
        "        rem = n % w\n",
        "        if rem == 0:\n",
        "            start0 = 0\n",
        "            end0_ex = n\n",
        "        else:\n",
        "            pad = rem // 2\n",
        "            start0 = pad\n",
        "            end0_ex = n - (rem - pad)\n",
        "\n",
        "        L = end0_ex - start0\n",
        "        if L < w:\n",
        "            continue\n",
        "\n",
        "        B = L // w  # number of full blocks\n",
        "        # Centered time indices (no intercept)\n",
        "        t = np.arange(1, w + 1, dtype=float)\n",
        "        t_centered = t - (w + 1) / 2.0\n",
        "        denom = np.dot(t_centered, t_centered)\n",
        "\n",
        "        y_seg = y[start0:end0_ex]\n",
        "        # reshape to (w x B) column-blocks, column-major like R's byrow=FALSE\n",
        "        y_mat = y_seg.reshape(w, B, order=\"F\")\n",
        "\n",
        "        has_na = ~np.isfinite(y_mat).all(axis=0)\n",
        "        num = t_centered @ y_mat  # (w,) @ (w,B) -> (B,)\n",
        "        slopes = num / denom\n",
        "        slopes[has_na] = np.nan\n",
        "\n",
        "        z = robust_z(slopes)\n",
        "        if not np.isfinite(z).any():\n",
        "            continue\n",
        "\n",
        "        vote_sign = np.zeros(B, dtype=int)\n",
        "        if direction == \"positive\":\n",
        "            vote_sign[z > mad_k] = 1\n",
        "        elif direction == \"negative\":\n",
        "            vote_sign[z < -mad_k] = 1  # presence of negative ramps counts as +1\n",
        "        else:  # both\n",
        "            vote_sign[z > mad_k] = 1\n",
        "            vote_sign[z < -mad_k] = -1\n",
        "\n",
        "        nz_blocks = np.nonzero(vote_sign != 0)[0]\n",
        "        if nz_blocks.size > 0:\n",
        "            for b in nz_blocks:\n",
        "                block_start = start0 + b * w\n",
        "                votes[block_start:block_start + w] += vote_sign[b]\n",
        "\n",
        "    if n_wls > 0:\n",
        "        votes = votes / n_wls\n",
        "    return votes"
      ],
      "metadata": {
        "id": "BSn6dOh5lOW9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detect_realtime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Literal, Union, Sequence, Optional\n",
        "\n",
        "\n",
        "def detect_realtime(\n",
        "    time: Sequence,\n",
        "    signal: Union[Sequence[float], np.ndarray],\n",
        "    lowwl: int = 5,\n",
        "    highwl: Union[str, int] = \"auto\",\n",
        "    mad_k: float = 3.0,\n",
        "    direction: Literal[\"positive\", \"both\", \"negative\"] = \"positive\",\n",
        "    burn_in: Optional[int] = None,\n",
        "    smooth_k: int = 30,\n",
        "    threshold: float = 1.3,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Real-time multi-scale slope voting (AS-votes) wrapper.\n",
        "    Computes offline detector on full series and real-time detector on prefixes.\n",
        "    \"\"\"\n",
        "    t = np.asarray(time)\n",
        "    y = np.asarray(signal, dtype=float).reshape(-1)\n",
        "    n = y.size\n",
        "    if t.shape[0] != n:\n",
        "        raise ValueError(\"`time` and `signal` lengths must match.\")\n",
        "\n",
        "    if n < 3:\n",
        "        return pd.DataFrame({\n",
        "            \"time_ind\": t,\n",
        "            \"signal\": y,\n",
        "            \"detected_offline\": np.zeros(n, dtype=float),\n",
        "            \"detected_value_rt\": np.zeros(n, dtype=float),\n",
        "            \"flag\": np.zeros(n, dtype=bool),\n",
        "            \"first_detection_time\": np.array([np.nan] * n, dtype=object),\n",
        "        })\n",
        "\n",
        "    if burn_in is None:\n",
        "        burn_in = max(100, 5 * int(lowwl))\n",
        "    burn_in = int(max(2, min(burn_in, n)))\n",
        "\n",
        "    offline = detect_asvotes(\n",
        "        signal=y,\n",
        "        lowwl=lowwl,\n",
        "        highwl=highwl,\n",
        "        mad_k=mad_k,\n",
        "        direction=direction,\n",
        "    )\n",
        "\n",
        "    rt = np.full(n, np.nan, dtype=float)\n",
        "\n",
        "    burn_highwl = \"auto\" if (isinstance(highwl, str) and highwl.lower() == \"auto\") else min(int(highwl), burn_in)\n",
        "    det_burn = detect_asvotes(\n",
        "        signal=y[:burn_in],\n",
        "        lowwl=lowwl,\n",
        "        highwl=burn_highwl,\n",
        "        mad_k=mad_k,\n",
        "        direction=direction,\n",
        "    )\n",
        "\n",
        "    if smooth_k > 0:\n",
        "        k0 = min(smooth_k, det_burn.size)\n",
        "        for i in range(burn_in):\n",
        "            j1 = max(0, i - k0 + 1)\n",
        "            rt[i] = np.nanmean(det_burn[j1:i+1])\n",
        "    else:\n",
        "        rt[:burn_in] = det_burn[:burn_in]\n",
        "\n",
        "    for i in range(burn_in, n):\n",
        "        cur_highwl = \"auto\" if (isinstance(highwl, str) and highwl.lower() == \"auto\") else min(int(highwl), i + 1)\n",
        "        det_i = detect_asvotes(\n",
        "            signal=y[:i+1],\n",
        "            lowwl=lowwl,\n",
        "            highwl=cur_highwl,\n",
        "            mad_k=mad_k,\n",
        "            direction=direction,\n",
        "        )\n",
        "        if smooth_k > 0:\n",
        "            k = min(smooth_k, det_i.size)\n",
        "            j1 = max(0, i - k + 1)\n",
        "            rt[i] = np.nanmean(det_i[j1:i+1])\n",
        "        else:\n",
        "            rt[i] = det_i[i]\n",
        "\n",
        "    flag_vec = (rt >= threshold)\n",
        "    if np.any(flag_vec):\n",
        "        first_time = t[np.argmax(flag_vec)]\n",
        "    else:\n",
        "        first_time = np.nan\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"time_ind\": t,\n",
        "        \"signal\": y,\n",
        "        \"detected_offline\": offline,\n",
        "        \"detected_value_rt\": rt,\n",
        "        \"flag\": flag_vec,\n",
        "        \"first_detection_time\": np.repeat(first_time, n).astype(object),\n",
        "    })\n",
        "    return out\n",
        "\n",
        "def write_rt_csv(df: pd.DataFrame, path: str) -> str:\n",
        "    \"\"\"\n",
        "    Write real-time detection output to CSV.\n",
        "    \"\"\"\n",
        "    df.to_csv(path, index=False)\n",
        "    return path"
      ],
      "metadata": {
        "id": "Mcvl_phslfuN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_detection_overlay\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional\n",
        "\n",
        "def plot_detection_overlay(\n",
        "    df: pd.DataFrame,\n",
        "    title: str = \"Real-time Detection vs Signal\",\n",
        "    time_col: str = \"time_ind\",\n",
        "    score_col: str = \"detected_value_rt\",\n",
        "    signal_col: str = \"signal\",\n",
        "    first_det_col: str = \"first_detection_time\",\n",
        "    score_label: str = \"Real-Time Detected Value\",\n",
        "    signal_label: str = \"Signal\",\n",
        "    x_label: str = \"Index\",\n",
        "    threshold: Optional[float] = None,\n",
        "    scale_factor: Optional[float] = None,\n",
        "    score_color: str = \"red\",\n",
        "    signal_color: str = \"black\",\n",
        "    save_path: Optional[str] = None,\n",
        "    width: float = 9,\n",
        "    height: float = 7.5,\n",
        "    dpi: int = 300,\n",
        "):\n",
        "    \"\"\"\n",
        "    Overlay real-time detection score and signal with threshold & first-crossing.\n",
        "\n",
        "    Creates a dual-axis plot with detection score (left y-axis) and the (optionally\n",
        "    scaled) signal plotted on the same axis but labeled on the right in original units.\n",
        "    \"\"\"\n",
        "    df_local = df.copy()\n",
        "\n",
        "    x = df_local[time_col].to_numpy()\n",
        "    sc = pd.to_numeric(df_local[score_col], errors=\"coerce\").to_numpy()\n",
        "    sg = pd.to_numeric(df_local[signal_col], errors=\"coerce\").to_numpy()\n",
        "\n",
        "    # Auto scale the signal onto the left axis range; right axis shows original via secondary label.\n",
        "    if scale_factor is None:\n",
        "        rng_sc = (np.nanmin(sc), np.nanmax(sc))\n",
        "        rng_sg = (np.nanmin(sg), np.nanmax(sg))\n",
        "        span_sc = rng_sc[1] - rng_sc[0]\n",
        "        span_sg = rng_sg[1] - rng_sg[0]\n",
        "        if not np.isfinite(span_sc) or span_sc == 0 or not np.isfinite(span_sg) or span_sg == 0:\n",
        "            alpha = 1.0\n",
        "        else:\n",
        "            alpha = float(span_sc / span_sg)\n",
        "    else:\n",
        "        alpha = float(scale_factor) if np.isfinite(scale_factor) and scale_factor != 0 else 1.0\n",
        "\n",
        "    scaled_signal = sg * alpha\n",
        "\n",
        "    # First detection time: constant across rows (may be NaN/None)\n",
        "    first_det_time = None\n",
        "    if first_det_col in df_local.columns:\n",
        "        vals = df_local[first_det_col].unique()\n",
        "        # Filter finite-like entries (works for numeric and datetime64; keep non-numeric non-null too)\n",
        "        cleaned = []\n",
        "        for v in vals:\n",
        "            if pd.isna(v):\n",
        "                continue\n",
        "            cleaned.append(v)\n",
        "        if cleaned:\n",
        "            first_det_time = cleaned[0]\n",
        "\n",
        "    # Build plot\n",
        "    fig, ax1 = plt.subplots(figsize=(width, height), dpi=dpi)\n",
        "\n",
        "    # Score on left axis\n",
        "    ax1.plot(x, sc, color=score_color, linewidth=1.2, label=score_label)\n",
        "    ax1.set_ylabel(score_label, color=score_color)\n",
        "    ax1.tick_params(axis='y', labelcolor=score_color)\n",
        "\n",
        "    # Scaled signal on same axis (plotted to match scale), with right-axis label in original units\n",
        "    ax1.plot(x, scaled_signal, color=signal_color, linewidth=1.0, alpha=0.85, label=signal_label)\n",
        "\n",
        "    # Threshold (horizontal) on score axis\n",
        "    if threshold is not None and np.isfinite(threshold):\n",
        "        ax1.axhline(y=threshold, linestyle=\"--\", color=score_color, linewidth=0.9)\n",
        "\n",
        "    # First detection time (vertical)\n",
        "    if first_det_time is not None:\n",
        "        ax1.axvline(x=first_det_time, linestyle=\":\", color=\"steelblue\", linewidth=1.0)\n",
        "\n",
        "    ax1.set_xlabel(x_label)\n",
        "    ax1.set_title(title)\n",
        "\n",
        "    # Secondary y-axis label to indicate original signal scale\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel(signal_label, color=signal_color)\n",
        "    ax2.tick_params(axis='y', labelcolor=signal_color)\n",
        "\n",
        "    # Sync right-axis ticks to represent original units\n",
        "    # y_left = alpha * signal  => signal = y_left / alpha\n",
        "    left_ticks = ax1.get_yticks()\n",
        "    ax2.set_yticks(left_ticks)\n",
        "    ax2.set_yticklabels([f\"{(yt / alpha):.3g}\" for yt in left_ticks])\n",
        "\n",
        "    # Simple legend\n",
        "    lines = ax1.get_lines()\n",
        "    labels = [score_label, signal_label]\n",
        "    ax1.legend(lines[:2], labels, loc=\"upper left\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path is not None:\n",
        "        fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "\n",
        "    return fig, ax1, ax2"
      ],
      "metadata": {
        "id": "iQCuxnSqmFBv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "myhn_vMAK-Ow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}